{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Personality Prediction Model\n",
        "\n",
        "This notebook develops a machine learning model to predict customer personality traits from marketing campaign data.\n",
        "\n",
        "## Problem Statement\n",
        "In competitive markets, businesses struggle to design marketing strategies that effectively target customers. This project aims to leverage advanced machine learning models to predict customer personality profiles based on demographic, behavioral, and purchase history data.\n",
        "\n",
        "## Objectives\n",
        "- Develop a machine learning model to predict customer personality traits\n",
        "- Improve marketing campaign effectiveness through targeted personalization\n",
        "- Provide actionable insights for marketing teams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Collection and Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('dataset_file.rtfd/marketing_campaign.csv.xls', sep='\\t')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nDataset Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle missing values in Income\n",
        "if df_processed['Income'].isnull().sum() > 0:\n",
        "    df_processed['Income'].fillna(df_processed['Income'].median(), inplace=True)\n",
        "\n",
        "# Convert Dt_Customer to datetime and extract features\n",
        "df_processed['Dt_Customer'] = pd.to_datetime(df_processed['Dt_Customer'], format='%d-%m-%Y')\n",
        "df_processed['Customer_Age'] = 2024 - df_processed['Year_Birth']\n",
        "df_processed['Days_Since_Customer'] = (pd.Timestamp('2024-01-01') - df_processed['Dt_Customer']).dt.days\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_processed = df_processed.drop(['ID', 'Year_Birth', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue'], axis=1)\n",
        "\n",
        "print(\"After preprocessing:\")\n",
        "print(f\"Shape: {df_processed.shape}\")\n",
        "print(f\"\\nMissing values: {df_processed.isnull().sum().sum()}\")\n",
        "df_processed.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create derived features\n",
        "df_processed['Total_Spent'] = (df_processed['MntWines'] + df_processed['MntFruits'] + \n",
        "                                df_processed['MntMeatProducts'] + df_processed['MntFishProducts'] + \n",
        "                                df_processed['MntSweetProducts'] + df_processed['MntGoldProds'])\n",
        "\n",
        "df_processed['Total_Purchases'] = (df_processed['NumDealsPurchases'] + df_processed['NumWebPurchases'] + \n",
        "                                    df_processed['NumCatalogPurchases'] + df_processed['NumStorePurchases'])\n",
        "\n",
        "df_processed['Total_Accepted_Campaigns'] = (df_processed['AcceptedCmp1'] + df_processed['AcceptedCmp2'] + \n",
        "                                             df_processed['AcceptedCmp3'] + df_processed['AcceptedCmp4'] + \n",
        "                                             df_processed['AcceptedCmp5'])\n",
        "\n",
        "df_processed['Avg_Purchase_Value'] = df_processed['Total_Spent'] / (df_processed['Total_Purchases'] + 1)\n",
        "df_processed['Children'] = df_processed['Kidhome'] + df_processed['Teenhome']\n",
        "df_processed['Family_Size'] = df_processed['Children'] + 1  # Assuming single person or couple\n",
        "\n",
        "# Spending patterns\n",
        "df_processed['Wine_Ratio'] = df_processed['MntWines'] / (df_processed['Total_Spent'] + 1)\n",
        "df_processed['Meat_Ratio'] = df_processed['MntMeatProducts'] / (df_processed['Total_Spent'] + 1)\n",
        "df_processed['Gold_Ratio'] = df_processed['MntGoldProds'] / (df_processed['Total_Spent'] + 1)\n",
        "\n",
        "# Purchase channel preferences\n",
        "df_processed['Web_Purchase_Ratio'] = df_processed['NumWebPurchases'] / (df_processed['Total_Purchases'] + 1)\n",
        "df_processed['Store_Purchase_Ratio'] = df_processed['NumStorePurchases'] / (df_processed['Total_Purchases'] + 1)\n",
        "df_processed['Catalog_Purchase_Ratio'] = df_processed['NumCatalogPurchases'] / (df_processed['Total_Purchases'] + 1)\n",
        "\n",
        "print(\"Feature engineering completed!\")\n",
        "print(f\"New shape: {df_processed.shape}\")\n",
        "df_processed.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables\n",
        "le_education = LabelEncoder()\n",
        "le_marital = LabelEncoder()\n",
        "\n",
        "df_processed['Education_Encoded'] = le_education.fit_transform(df_processed['Education'])\n",
        "df_processed['Marital_Status_Encoded'] = le_marital.fit_transform(df_processed['Marital_Status'])\n",
        "\n",
        "# Drop original categorical columns\n",
        "df_processed = df_processed.drop(['Education', 'Marital_Status'], axis=1)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop('Response', axis=1)\n",
        "y = df_processed['Response']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "print(f\"\\nTarget distribution %:\\n{y.value_counts(normalize=True) * 100}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"\\nTraining target distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"\\nTest target distribution:\\n{y_test.value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Development\n",
        "\n",
        "We'll train and compare multiple models:\n",
        "1. Random Forest\n",
        "2. XGBoost\n",
        "3. LightGBM\n",
        "4. Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    precision = precision_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
        "    \n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='roc_auc')\n",
        "    \n",
        "    results = {\n",
        "        'Model': model_name,\n",
        "        'Train_Accuracy': train_accuracy,\n",
        "        'Test_Accuracy': test_accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1_Score': f1,\n",
        "        'ROC_AUC': roc_auc,\n",
        "        'CV_ROC_AUC_Mean': cv_scores.mean(),\n",
        "        'CV_ROC_AUC_Std': cv_scores.std()\n",
        "    }\n",
        "    \n",
        "    return model, results, y_test_pred, y_test_proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=5)\n",
        "rf_trained, rf_results, rf_pred, rf_proba = evaluate_model(\n",
        "    rf_model, X_train, X_test, y_train, y_test, 'Random Forest'\n",
        ")\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "for key, value in rf_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 XGBoost Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1)\n",
        "xgb_trained, xgb_results, xgb_pred, xgb_proba = evaluate_model(\n",
        "    xgb_model, X_train, X_test, y_train, y_test, 'XGBoost'\n",
        ")\n",
        "\n",
        "print(\"XGBoost Results:\")\n",
        "for key, value in xgb_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 LightGBM Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42, max_depth=6, learning_rate=0.1, verbose=-1)\n",
        "lgb_trained, lgb_results, lgb_pred, lgb_proba = evaluate_model(\n",
        "    lgb_model, X_train, X_test, y_train, y_test, 'LightGBM'\n",
        ")\n",
        "\n",
        "print(\"LightGBM Results:\")\n",
        "for key, value in lgb_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Neural Network (MLP Classifier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Neural Network (using scaled data)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, alpha=0.01)\n",
        "nn_trained, nn_results, nn_pred, nn_proba = evaluate_model(\n",
        "    nn_model, X_train_scaled, X_test_scaled, y_train, y_test, 'Neural Network'\n",
        ")\n",
        "\n",
        "print(\"Neural Network Results:\")\n",
        "for key, value in nn_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "results_df = pd.DataFrame([rf_results, xgb_results, lgb_results, nn_results])\n",
        "results_df = results_df.set_index('Model')\n",
        "print(\"Model Comparison:\")\n",
        "print(results_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "results_df[['Test_Accuracy', 'Train_Accuracy']].plot(kind='bar', ax=axes[0, 0], title='Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Precision, Recall, F1\n",
        "results_df[['Precision', 'Recall', 'F1_Score']].plot(kind='bar', ax=axes[0, 1], title='Precision, Recall, F1 Comparison')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# ROC-AUC\n",
        "results_df[['ROC_AUC', 'CV_ROC_AUC_Mean']].plot(kind='bar', ax=axes[1, 0], title='ROC-AUC Comparison')\n",
        "axes[1, 0].set_ylabel('ROC-AUC')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Cross-validation scores\n",
        "results_df['CV_ROC_AUC_Mean'].plot(kind='bar', ax=axes[1, 1], title='Cross-Validation ROC-AUC Mean', color='green')\n",
        "axes[1, 1].set_ylabel('CV ROC-AUC Mean')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Select Best Model and Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model based on ROC-AUC\n",
        "best_model_name = results_df['ROC_AUC'].idxmax()\n",
        "print(f\"Best model: {best_model_name} with ROC-AUC: {results_df.loc[best_model_name, 'ROC_AUC']:.4f}\")\n",
        "\n",
        "# Get the best model\n",
        "if best_model_name == 'Random Forest':\n",
        "    best_model = rf_trained\n",
        "    use_scaled = False\n",
        "elif best_model_name == 'XGBoost':\n",
        "    best_model = xgb_trained\n",
        "    use_scaled = False\n",
        "elif best_model_name == 'LightGBM':\n",
        "    best_model = lgb_trained\n",
        "    use_scaled = False\n",
        "else:\n",
        "    best_model = nn_trained\n",
        "    use_scaled = True\n",
        "\n",
        "print(f\"\\nBest model details:\")\n",
        "print(results_df.loc[best_model_name])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model and preprocessing objects\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, 'models/best_model.pkl')\n",
        "joblib.dump(scaler, 'models/scaler.pkl')\n",
        "joblib.dump(le_education, 'models/le_education.pkl')\n",
        "joblib.dump(le_marital, 'models/le_marital.pkl')\n",
        "\n",
        "# Save feature names\n",
        "import json\n",
        "with open('models/feature_names.json', 'w') as f:\n",
        "    json.dump(list(X.columns), f)\n",
        "\n",
        "# Save model metadata\n",
        "model_metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'use_scaled': use_scaled,\n",
        "    'roc_auc': float(results_df.loc[best_model_name, 'ROC_AUC']),\n",
        "    'accuracy': float(results_df.loc[best_model_name, 'Test_Accuracy']),\n",
        "    'precision': float(results_df.loc[best_model_name, 'Precision']),\n",
        "    'recall': float(results_df.loc[best_model_name, 'Recall']),\n",
        "    'f1_score': float(results_df.loc[best_model_name, 'F1_Score'])\n",
        "}\n",
        "\n",
        "with open('models/model_metadata.json', 'w') as f:\n",
        "    json.dump(model_metadata, f, indent=2)\n",
        "\n",
        "print(\"Model and preprocessing objects saved successfully!\")\n",
        "print(f\"\\nModel metadata:\\n{json.dumps(model_metadata, indent=2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
        "    plt.title('Top 15 Feature Importances')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Top 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Confusion Matrix and Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions from best model\n",
        "if use_scaled:\n",
        "    y_pred_best = best_model.predict(X_test_scaled)\n",
        "else:\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Response', 'Response'], \n",
        "            yticklabels=['No Response', 'Response'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nClassification Report - {best_model_name}:\")\n",
        "print(classification_report(y_test, y_pred_best, target_names=['No Response', 'Response']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ✅ Loaded and explored the marketing campaign dataset\n",
        "2. ✅ Preprocessed the data (handled missing values, encoded categorical variables)\n",
        "3. ✅ Created derived features (spending patterns, purchase behavior, etc.)\n",
        "4. ✅ Trained multiple models (Random Forest, XGBoost, LightGBM, Neural Network)\n",
        "5. ✅ Evaluated models using multiple metrics (Accuracy, Precision, Recall, F1, ROC-AUC)\n",
        "6. ✅ Selected the best model and saved it for deployment\n",
        "7. ✅ Analyzed feature importance and model performance\n",
        "\n",
        "The model is now ready for deployment in the web application!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
